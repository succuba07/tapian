# -*- coding: utf-8 -*-
"""
ç”²éª¨æ–‡æ‹“ç‰‡éª¨æ¶æå–ç³»ç»Ÿ - å¢å¼ºç‰ˆï¼ˆé›†æˆæ™ºèƒ½å™ªå£°åˆ†ç¦»ï¼‰
æ ¸å¿ƒå‡çº§ï¼š
1. æ™ºèƒ½å™ªå£°åˆ†ç¦»ï¼šåœ¨æ¸…æ™°åº¦è¯„ä¼°å‰å»é™¤å¤–æºæ€§è®¾å¤‡å™ªå£°ï¼Œä¿ç•™å†…æºæ€§æ‹“ç‰‡ç‰¹å¾
2. å¤šç»´åº¦æ¸…æ™°åº¦è¯„ä¼°ï¼šæ¢¯åº¦åŸŸï¼ˆæŠ—å™ªå£°ï¼‰+é¢‘åŸŸï¼ˆå°ºåº¦ä¸å˜ï¼‰+å±€éƒ¨è‡ªé€‚åº”ï¼ˆè§£å†³å¯¹æ¯”åº¦å·®å¼‚ï¼‰
3. è¯¦ç»†è¯Šæ–­æŠ¥å‘Šï¼šæä¾›å™ªå£°å¤„ç†åˆ†æå’Œæ”¹è¿›å»ºè®®
"""

import numpy as np
import cv2
import os
from datetime import datetime
import glob
import pandas as pd
import logging
import matplotlib.pyplot as plt
from scipy import ndimage

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class IntelligentNoiseProcessor:
    """æ™ºèƒ½å™ªå£°å¤„ç†å™¨ - ä¸“é—¨åŒºåˆ†å’Œå¤„ç†å¤–æºæ€§å™ªå£°"""
    
    def __init__(self):
        # å¤–æºæ€§å™ªå£°ç‰¹å¾å‚æ•°ï¼ˆå¯æ ¹æ®å®é™…æ•°æ®è°ƒæ•´ï¼‰
        self.exogenous_params = {
            'max_size': 10,           # æœ€å¤§å™ªå£°ç‚¹å°ºå¯¸ï¼ˆåƒç´ ï¼‰
            'intensity_threshold': 160,  # å™ªå£°ç‚¹äº®åº¦é˜ˆå€¼
            'min_isolation': 0.5,    # æœ€å°å­¤ç«‹æ€§é˜ˆå€¼
            'morph_kernel_size': 2   # å½¢æ€å­¦æ“ä½œæ ¸å¤§å°
        }
        
    def remove_exogenous_noise(self, image_path):
        """
        å»é™¤å¤–æºæ€§å™ªå£°ä½†ä¿ç•™å†…æºæ€§ç‰¹å¾
        è¿”å›: (å»å™ªåçš„å›¾åƒ, å™ªå£°æ©ç )
        """
        try:
            # è¯»å–å›¾åƒ
            if isinstance(image_path, str):
                img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
            else:
                img = image_path.copy()
                
            if img is None:
                return None, None
            
            # æ£€æµ‹å¤–æºæ€§å™ªå£°
            noise_mask = self._detect_exogenous_noise(img)
            
            # åº”ç”¨ä¸­å€¼æ»¤æ³¢ï¼Œä½†ä»…é™äºå™ªå£°åŒºåŸŸ
            denoised = self._selective_median_filter(img, noise_mask)
            
            return denoised, noise_mask
            
        except Exception as e:
            logging.error(f"å™ªå£°å¤„ç†å¤±è´¥: {str(e)}")
            return None, None
        except KeyError as e:
            logging.error(f"å‚æ•°é…ç½®é”™è¯¯: {str(e)}ï¼Œè¯·æ£€æŸ¥exogenous_paramså­—å…¸ä¸­çš„é”®å")
            return None, None
        except AttributeError as e:
            logging.error(f"OpenCVå‡½æ•°è°ƒç”¨é”™è¯¯: {str(e)}ï¼Œè¯·æ£€æŸ¥OpenCVç‰ˆæœ¬å’Œå‡½æ•°å")
            return None, None
        except Exception as e:
            logging.error(f"å™ªå£°å¤„ç†å¤±è´¥: {str(e)}")
            return None, None
    
    def _detect_exogenous_noise(self, gray_image):
        """æ£€æµ‹å¤–æºæ€§å™ªå£°ç‰¹å¾"""
        height, width = gray_image.shape
        noise_mask = np.zeros_like(gray_image, dtype=np.uint8)
        
        # åŸºäºè¿é€šç»„ä»¶åˆ†æ
        _, binary = cv2.threshold(gray_image, self.exogenous_params['intensity_threshold'], 
                                255, cv2.THRESH_BINARY)
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary, connectivity=8)
        
        for i in range(1, num_labels):  # è·³è¿‡èƒŒæ™¯
            # æ£€æŸ¥ç»„ä»¶ç‰¹å¾
            area = stats[i, cv2.CC_STAT_AREA]
            width = stats[i, cv2.CC_STAT_WIDTH]
            height = stats[i, cv2.CC_STAT_HEIGHT]
            
            # å¤–æºæ€§å™ªå£°åˆ¤æ–­æ¡ä»¶
            if (area <= self.exogenous_params['max_size'] ** 2 and  # å°é¢ç§¯
                max(width, height) <= self.exogenous_params['max_size'] and  # å°å°ºå¯¸
                self._is_isolated_component(labels == i, labels)):  # å­¤ç«‹æ€§
                
                # æ ‡è®°ä¸ºå¤–æºæ€§å™ªå£°
                noise_mask[labels == i] = 255
        
        # å½¢æ€å­¦æ“ä½œå¢å¼ºæ£€æµ‹
        kernel = np.ones((self.exogenous_params['morph_kernel_size'], 
                         self.exogenous_params['morph_kernel_size']), np.uint8)
        noise_mask = cv2.morphologyEx(noise_mask, cv2.MORPH_CLOSE, kernel)
        
        return noise_mask
    
    def _is_isolated_component(self, component_mask, all_labels):
        """æ£€æŸ¥ç»„ä»¶æ˜¯å¦å­¤ç«‹ï¼ˆä¸ä¸ä¸»è¦ç»“æ„è¿æ¥ï¼‰"""
        # è†¨èƒ€ç»„ä»¶æ£€æŸ¥é‚»æ¥å…³ç³»
        kernel = np.ones((3, 3), np.uint8)
        dilated = cv2.dilate(component_mask.astype(np.uint8), kernel)
        
        # æ£€æŸ¥è†¨èƒ€åæ˜¯å¦æ¥è§¦å…¶ä»–å¤§ç»„ä»¶
        dilated_labels = dilated * all_labels
        unique_neighbors = np.unique(dilated_labels[dilated > 0])
        
        # å¦‚æœåªæ¥è§¦è‡ªå·±æˆ–èƒŒæ™¯ï¼Œåˆ™æ˜¯å­¤ç«‹çš„
        return len(unique_neighbors) <= 2
    
    def _selective_median_filter(self, image, noise_mask, kernel_size=3):
        """é€‰æ‹©æ€§ä¸­å€¼æ»¤æ³¢ - ä»…åœ¨å™ªå£°åŒºåŸŸåº”ç”¨"""
        # å¯¹æ•´ä¸ªå›¾åƒè¿›è¡Œè½»åº¦æ»¤æ³¢
        lightly_filtered = cv2.medianBlur(image, kernel_size)
        
        # åªåœ¨æ£€æµ‹åˆ°çš„å™ªå£°åŒºåŸŸåº”ç”¨æ»¤æ³¢ç»“æœ
        result = image.copy()
        result[noise_mask == 255] = lightly_filtered[noise_mask == 255]
        
        return result

class EnhancedClarityEvaluator:
    """å¢å¼ºçš„æ¸…æ™°åº¦è¯„ä¼°å™¨ - é›†æˆæ™ºèƒ½å™ªå£°å»é™¤åŠŸèƒ½"""
    
    def __init__(self):
        # æƒé‡é…ç½®ï¼ˆå¯æ ¹æ®å®é™…æ•°æ®è°ƒæ•´ï¼‰
        self.weights = {
            'gradient': 0.5,    # æ¢¯åº¦ç‰¹å¾æƒé‡ï¼ˆæŠ—å™ªå£°ï¼‰
            'frequency': 0.25,   # é¢‘åŸŸç‰¹å¾æƒé‡ï¼ˆå°ºåº¦ä¸å˜ï¼‰
            'local': 0.25        # å±€éƒ¨ç‰¹å¾æƒé‡ï¼ˆé€‚åº”å¯¹æ¯”åº¦å·®å¼‚ï¼‰
        }
        # æ–°å¢ï¼šé›†æˆæ™ºèƒ½å™ªå£°å¤„ç†å™¨
        self.noise_processor = IntelligentNoiseProcessor()
        
    def calculate_clarity_score(self, image_path):
        """
        ç»¼åˆæ¸…æ™°åº¦è¯„ä¼°ï¼ˆå…ˆå»é™¤å¤–æºæ€§å™ªå£°ï¼‰
        è¿”å›: ç»¼åˆå¾—åˆ†(0-100)ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # è¯»å–å›¾åƒå¹¶å»é™¤å¤–æºæ€§å™ªå£°
            denoised_img, noise_mask = self.noise_processor.remove_exogenous_noise(image_path)
            if denoised_img is None:
                logging.error(f"å›¾åƒå»å™ªå¤±è´¥: {image_path}")
                return None
            
            # åœ¨å»å™ªåçš„å›¾åƒä¸Šå¹¶è¡Œè®¡ç®—ä¸‰ç§ç‰¹å¾
            gradient_score = self._gradient_based_sharpness(denoised_img)
            frequency_score = self._frequency_domain_sharpness(denoised_img)
            local_score = self._adaptive_local_sharpness(denoised_img)
            
            # å½’ä¸€åŒ–å¤„ç†
            scores = {
                'gradient': self._normalize_score(gradient_score, (0, 100)),
                'frequency': self._normalize_score(frequency_score, (0, 100)),
                'local': self._normalize_score(local_score, (0, 5000))
            }
            
            # åŠ æƒç»¼åˆå¾—åˆ†
            total_score = sum(scores[method] * weight 
                             for method, weight in self.weights.items())
            
            return round(total_score, 2)
            
        except Exception as e:
            logging.error(f"ç»¼åˆæ¸…æ™°åº¦è¯„ä¼°å¤±è´¥: {str(e)}")
            return None
    
    def calculate_detailed_assessment(self, image_path):
        """
        è¯¦ç»†çš„æ¸…æ™°åº¦è¯„ä¼°ï¼ˆè¿”å›å®Œæ•´åˆ†ææŠ¥å‘Šï¼ŒåŒ…å«å™ªå£°å¤„ç†ä¿¡æ¯ï¼‰
        """
        try:
            # è¯»å–å›¾åƒå¹¶å»é™¤å¤–æºæ€§å™ªå£°
            denoised_img, noise_mask = self.noise_processor.remove_exogenous_noise(image_path)
            if denoised_img is None:
                return None
            
            # è®¡ç®—å™ªå£°å»é™¤æ¯”ä¾‹ï¼ˆç”¨äºè¯Šæ–­æŠ¥å‘Šï¼‰
            noise_ratio = np.sum(noise_mask) / (noise_mask.size * 255) if noise_mask is not None else 0
            
            # åœ¨å»å™ªåçš„å›¾åƒä¸Šè®¡ç®—æ¸…æ™°åº¦
            gradient_score = self._gradient_based_sharpness(denoised_img)
            frequency_score = self._frequency_domain_sharpness(denoised_img)
            local_score = self._adaptive_local_sharpness(denoised_img)
            
            # å½’ä¸€åŒ–
            scores = {
                'gradient': self._normalize_score(gradient_score, (0, 100)),
                'frequency': self._normalize_score(frequency_score, (0, 100)),
                'local': self._normalize_score(local_score, (0, 5000))
            }
            
            # åŠ æƒç»¼åˆ
            total_score = sum(scores[method] * weight 
                             for method, weight in self.weights.items())
            
            # é—®é¢˜è¯Šæ–­ï¼ˆåŸºäºå»å™ªåå›¾åƒï¼‰
            diagnosis = self._diagnose_issues(scores)
            
            # æ·»åŠ å™ªå£°å¤„ç†ä¿¡æ¯åˆ°è¯Šæ–­ç»“æœ
            diagnosis.append(f"å¤–æºæ€§å™ªå£°å»é™¤æ¯”ä¾‹: {noise_ratio:.1%}")
            
            return {
                'total_score': round(total_score, 2),
                'detailed_scores': scores,
                'diagnosis': diagnosis,
                'recommendation': self._generate_recommendation(total_score, diagnosis),
                'noise_removed_ratio': noise_ratio,
                'assessment_note': 'è¯„ä¼°åŸºäºå»å™ªåå›¾åƒï¼Œå¤–æºæ€§å™ªå£°å·²å»é™¤'
            }
            
        except Exception as e:
            logging.error(f"è¯¦ç»†æ¸…æ™°åº¦è¯„ä¼°å¤±è´¥: {str(e)}")
            return None
    
    def _gradient_based_sharpness(self, image):
        """
        åŸºäºæ¢¯åº¦åŸŸçš„æ¸…æ™°åº¦è¯„ä¼°ï¼ˆæŠ—å™ªå£°å¹²æ‰°ï¼‰
        """
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image
        
        # å¤šæ–¹å‘æ¢¯åº¦è®¡ç®—
        sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        
        # æ¢¯åº¦å¹…å€¼ï¼ˆè¾¹ç¼˜å¼ºåº¦ï¼‰
        gradient_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)
        
        # é«˜æ¢¯åº¦åƒç´ æ¯”ä¾‹ï¼ˆçœŸæ­£çš„æ–‡å­—è¾¹ç¼˜ï¼‰
        high_gradient_ratio = np.sum(gradient_magnitude > np.percentile(gradient_magnitude, 90)) / gradient_magnitude.size
        
        return high_gradient_ratio * 100
    
    def _frequency_domain_sharpness(self, image):
        """
        é¢‘åŸŸæ¸…æ™°åº¦è¯„ä¼°ï¼ˆå°ºåº¦ä¸å˜æ€§ï¼‰
        """
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image
        
        # å‚…é‡Œå¶å˜æ¢
        f = np.fft.fft2(gray)
        fshift = np.fft.fftshift(f)
        
        # é¢‘åŸŸå¹…åº¦è°±
        magnitude_spectrum = np.log(np.abs(fshift) + 1)
        
        # é«˜é¢‘èƒ½é‡å æ¯”ï¼ˆä¸å›¾åƒå°ºåº¦æ— å…³ï¼‰
        rows, cols = gray.shape
        crow, ccol = rows//2, cols//2
        
        # åˆ›å»ºç¯å½¢æ©ç æå–é«˜é¢‘æˆåˆ†
        high_freq_mask = np.zeros((rows, cols))
        for r in range(rows):
            for c in range(cols):
                dist = np.sqrt((r - crow)**2 + (c - ccol)**2)
                if dist > min(rows, cols) * 0.3:  # é«˜é¢‘åŒºåŸŸ
                    high_freq_mask[r, c] = 1
        
        high_freq_energy = np.sum(magnitude_spectrum * high_freq_mask)
        total_energy = np.sum(magnitude_spectrum)
        
        return (high_freq_energy / total_energy) * 100 if total_energy > 0 else 0
    
    def _adaptive_local_sharpness(self, image, block_size=32):
        """
        è‡ªé€‚åº”å±€éƒ¨æ¸…æ™°åº¦è¯„ä¼°ï¼ˆè§£å†³å¯¹æ¯”åº¦å·®å¼‚ï¼‰
        """
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image
        
        height, width = gray.shape
        
        sharpness_scores = []
        
        # åˆ†å—å¤„ç†
        for i in range(0, height, block_size):
            for j in range(0, width, block_size):
                block = gray[i:min(i+block_size, height), j:min(j+block_size, width)]
                
                if block.size > 100:  # ç¡®ä¿å—è¶³å¤Ÿå¤§
                    # è®¡ç®—å—çš„å±€éƒ¨æ¸…æ™°åº¦ï¼ˆä½¿ç”¨Laplacianæ–¹å·®ï¼‰
                    laplacian_var = cv2.Laplacian(block, cv2.CV_64F).var()
                    if not np.isnan(laplacian_var):
                        sharpness_scores.append(laplacian_var)
        
        # ä½¿ç”¨ä¸­ä½æ•°é¿å…æç«¯å€¼å½±å“
        return np.median(sharpness_scores) if sharpness_scores else 0
    
    def _normalize_score(self, value, value_range):
        """å½’ä¸€åŒ–åˆ†æ•°åˆ°0-100èŒƒå›´"""
        min_val, max_val = value_range
        if max_val - min_val == 0:
            return 0
        normalized = (value - min_val) / (max_val - min_val) * 100
        return max(0, min(100, normalized))
    
    def _diagnose_issues(self, scores):
        """é—®é¢˜è¯Šæ–­ï¼ˆåŸºäºå»å™ªåå›¾åƒï¼‰"""
        issues = []
        
        if scores['gradient'] < 30:
            issues.append("å™ªå£°å¹²æ‰°è¾ƒä¸¥é‡ï¼ˆå»å™ªåè¯„ä¼°ï¼‰")
        if scores['frequency'] < 25:
            issues.append("å›¾åƒå¯èƒ½è¿‡åº¦æ¨¡ç³Šæˆ–ç¼©æ”¾ä¸ä¸€è‡´")
        if scores['local'] < 20:
            issues.append("å±€éƒ¨å¯¹æ¯”åº¦å·®å¼‚æ˜æ˜¾")
        
        return issues if issues else ["å›¾åƒè´¨é‡è‰¯å¥½"]
    
    def _generate_recommendation(self, total_score, issues):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®ï¼ˆåŸºäºå»å™ªåè¯„ä¼°ï¼‰"""
        if total_score >= 80:
            return "å›¾åƒè´¨é‡ä¼˜ç§€ï¼Œæ— éœ€è¿›ä¸€æ­¥å¤„ç†"
        elif total_score >= 60:
            return "å»ºè®®è½»åº¦å¯¹æ¯”åº¦å¢å¼º"
        else:
            return "éœ€è¦ç»¼åˆå¤„ç†ï¼šå¯¹æ¯”åº¦å¢å¼º+å°ºå¯¸æ ‡å‡†åŒ–"

class OracleBoneSkeletonExtractor:
    """ç”²éª¨æ–‡éª¨æ¶æå–å™¨ - ä¸“é—¨ç”¨äºç”Ÿæˆtest_skeletonå›¾"""
    
    def __init__(self, min_area=70, smoothing=True):
        self.min_area = min_area
        self.smoothing = smoothing
        
    def extract_skeleton(self, image_path):
        """ä»å•å¼ å›¾åƒæå–éª¨æ¶"""
        try:
            # è¯»å–å›¾åƒ
            if isinstance(image_path, str):
                image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
            else:
                image = image_path
                
            if image is None:
                return None
            
            # ä¸­å€¼æ»¤æ³¢é¢„å¤„ç†
            denoised = cv2.medianBlur(image, 3)
            
            # è‡ªé€‚åº”äºŒå€¼åŒ–
            _, binary = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            
            # å½¢æ€å­¦æ“ä½œå¢å¼ºè¿é€šæ€§
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
            
            # æå–è½®å»“
            contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # åˆ›å»ºè½®å»“æ©ç 
            contour_mask = np.zeros_like(binary)
            valid_contours = [cnt for cnt in contours if cv2.contourArea(cnt) >= self.min_area]
            cv2.drawContours(contour_mask, valid_contours, -1, 255, -1)
            
            # å¹³æ»‘å¤„ç†
            if self.smoothing:
                contour_mask = self._smooth_contour(contour_mask)
            
            return contour_mask
            
        except Exception as e:
            logging.error(f"å¤„ç†å›¾åƒæ—¶å‡ºé”™: {e}")
            return None
    
    def _smooth_contour(self, contour_mask):
        """å¹³æ»‘è½®å»“æ©ç """
        # å½¢æ€å­¦é—­æ“ä½œå¡«å……å°å­”
        kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        smoothed = cv2.morphologyEx(contour_mask, cv2.MORPH_CLOSE, kernel_close)
        
        # å½¢æ€å­¦å¼€æ“ä½œå»é™¤æ¯›åˆº
        kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        smoothed = cv2.morphologyEx(smoothed, cv2.MORPH_OPEN, kernel_open)
        
        return smoothed

class EnhancedBatchProcessor:
    """å¢å¼ºçš„æ‰¹é‡å¤„ç†å™¨ - æ”¯æŒç»¼åˆæ¸…æ™°åº¦è¯„ä¼°"""
    
    def __init__(self, input_dir, output_dir):
        self.input_dir = input_dir
        self.output_dir = output_dir
        self.clarity_evaluator = EnhancedClarityEvaluator()
        self.skeleton_extractor = OracleBoneSkeletonExtractor(min_area=70, smoothing=True)
        self.supported_formats = ['.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp']
        
    def process_clarity_evaluation(self):
        """æ‰¹é‡å¤„ç†å¢å¼ºç‰ˆæ¸…æ™°åº¦è¯„ä¼°ï¼ˆé›†æˆå™ªå£°å»é™¤ï¼‰"""
        if not os.path.exists(self.input_dir):
            raise ValueError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {self.input_dir}")
        
        os.makedirs(self.output_dir, exist_ok=True)
        
        stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'failed_files': []
        }
        
        results = []
        
        print(f"å¼€å§‹å¢å¼ºç‰ˆæ¸…æ™°åº¦è¯„ä¼°: {self.input_dir}")
        print("=" * 60)
        print("è¯„ä¼°ç­–ç•¥: å»é™¤å¤–æºæ€§è®¾å¤‡å™ªå£°ï¼Œä¿ç•™å†…æºæ€§æ‹“ç‰‡ç‰¹å¾")
        print("=" * 60)
        
        # éå†æ‰€æœ‰å­ç›®å½•
        for root, dirs, files in os.walk(self.input_dir):
            # è®¡ç®—ç›¸å¯¹è·¯å¾„
            rel_path = os.path.relpath(root, self.input_dir)
            
            # å¤„ç†å½“å‰ç›®å½•çš„æ–‡ä»¶
            for file in files:
                if self._is_image_file(file):
                    input_path = os.path.join(root, file)
                    stats['total'] += 1
                    
                    # ä½¿ç”¨æ–°æ–¹æ³•è®¡ç®—æ¸…æ™°åº¦å¾—åˆ†ï¼ˆé›†æˆå™ªå£°å»é™¤ï¼‰
                    clarity_result = self.clarity_evaluator.calculate_detailed_assessment(input_path)
                    
                    if clarity_result is not None:
                        # è®°å½•è¯¦ç»†ç»“æœï¼ˆåŒ…å«å™ªå£°å¤„ç†ä¿¡æ¯ï¼‰
                        results.append({
                            'filename': file,
                            'subfolder': rel_path,
                            'total_score': clarity_result['total_score'],
                            'gradient_score': clarity_result['detailed_scores']['gradient'],
                            'frequency_score': clarity_result['detailed_scores']['frequency'],
                            'local_score': clarity_result['detailed_scores']['local'],
                            'noise_removed_ratio': clarity_result['noise_removed_ratio'],
                            'diagnosis': ';'.join(clarity_result['diagnosis']),
                            'recommendation': clarity_result['recommendation']
                        })
                        stats['success'] += 1
                        print(f"âœ… æˆåŠŸ: {file} | å™ªå£°å»é™¤: {clarity_result['noise_removed_ratio']:.1%} | æ¸…æ™°åº¦: {clarity_result['total_score']:.2f}")
                    else:
                        stats['failed'] += 1
                        stats['failed_files'].append(input_path)
                        print(f"âŒ å¤±è´¥: {file}")
        
        # ä¿å­˜å¢å¼ºç‰ˆæ¸…æ™°åº¦å¾—åˆ†ç»“æœ
        if results:
            self._save_enhanced_clarity_results(results)
        
        # ç”Ÿæˆå¢å¼ºç‰ˆæŠ¥å‘Š
        self._generate_enhanced_clarity_report(stats, results)
        return stats
    
    def process_skeleton_extraction(self):
        """æ‰¹é‡å¤„ç†éª¨æ¶æå–"""
        if not os.path.exists(self.input_dir):
            raise ValueError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {self.input_dir}")
        
        os.makedirs(self.output_dir, exist_ok=True)
        
        stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'failed_files': []
        }
        
        print(f"å¼€å§‹éª¨æ¶æå–: {self.input_dir}")
        print("=" * 50)
        
        # éå†æ‰€æœ‰å­ç›®å½•
        for root, dirs, files in os.walk(self.input_dir):
            # è®¡ç®—ç›¸å¯¹è·¯å¾„
            rel_path = os.path.relpath(root, self.input_dir)
            
            # åˆ›å»ºå¯¹åº”çš„è¾“å‡ºå­ç›®å½•
            if rel_path != '.':
                output_subdir = os.path.join(self.output_dir, rel_path)
                os.makedirs(output_subdir, exist_ok=True)
            else:
                output_subdir = self.output_dir
            
            # å¤„ç†å½“å‰ç›®å½•çš„æ–‡ä»¶
            for file in files:
                if self._is_image_file(file):
                    input_path = os.path.join(root, file)
                    stats['total'] += 1
                    
                    # å¤„ç†å›¾åƒ
                    skeleton = self.skeleton_extractor.extract_skeleton(input_path)
                    
                    if skeleton is not None:
                        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆä¿æŒåŸåï¼Œæ·»åŠ _skeletonåç¼€ï¼‰
                        output_filename = self._get_output_filename(file)
                        output_path = os.path.join(output_subdir, output_filename)
                        
                        # ä¿å­˜éª¨æ¶å›¾
                        cv2.imwrite(output_path, skeleton)
                        stats['success'] += 1
                        print(f"âœ… æˆåŠŸ: {file} -> {output_filename}")
                    else:
                        stats['failed'] += 1
                        stats['failed_files'].append(input_path)
                        print(f"âŒ å¤±è´¥: {file}")
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_skeleton_report(stats)
        return stats
    
    def _is_image_file(self, filename):
        """æ£€æŸ¥æ˜¯å¦ä¸ºå›¾åƒæ–‡ä»¶"""
        ext = os.path.splitext(filename)[1].lower()
        return ext in self.supported_formats
    
    def _get_output_filename(self, filename):
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å"""
        name, ext = os.path.splitext(filename)
        return f"{name}.png"
    
    def _save_enhanced_clarity_results(self, results):
        """ä¿å­˜å¢å¼ºç‰ˆæ¸…æ™°åº¦ç»“æœ"""
        df = pd.DataFrame(results)
        csv_path = os.path.join(self.output_dir, "enhanced_clarity_scores.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        logging.info(f"å¢å¼ºç‰ˆæ¸…æ™°åº¦å¾—åˆ†å·²ä¿å­˜åˆ°: {csv_path}")
        
        # æ‰“å°ç»Ÿè®¡æ‘˜è¦
        if not df.empty:
            print("\nå¢å¼ºç‰ˆæ¸…æ™°åº¦è¯„ä¼°ç»Ÿè®¡æ‘˜è¦:")
            print(f"å¹³å‡ç»¼åˆå¾—åˆ†: {df['total_score'].mean():.2f}")
            print(f"å¹³å‡æ¢¯åº¦å¾—åˆ†: {df['gradient_score'].mean():.2f}")
            print(f"å¹³å‡é¢‘åŸŸå¾—åˆ†: {df['frequency_score'].mean():.2f}")
            print(f"å¹³å‡å±€éƒ¨å¾—åˆ†: {df['local_score'].mean():.2f}")
            print(f"å¹³å‡å™ªå£°å»é™¤æ¯”ä¾‹: {df['noise_removed_ratio'].mean():.1%}")
            
            # å¸¸è§é—®é¢˜ç»Ÿè®¡
            diagnosis_counts = df['diagnosis'].value_counts()
            print("\nå¸¸è§é—®é¢˜ç»Ÿè®¡:")
            for diagnosis, count in diagnosis_counts.items():
                print(f"  {diagnosis}: {count}æ¬¡")
    
    def _generate_enhanced_clarity_report(self, stats, results):
        """ç”Ÿæˆå¢å¼ºç‰ˆæ¸…æ™°åº¦è¯„ä¼°æŠ¥å‘Š"""
        report_path = os.path.join(self.output_dir, "enhanced_clarity_report.txt")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("ç”²éª¨æ–‡æ‹“ç‰‡å¢å¼ºç‰ˆæ¸…æ™°åº¦è¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è¾“å…¥ç›®å½•: {self.input_dir}\n")
            f.write(f"è¾“å‡ºç›®å½•: {self.output_dir}\n\n")
            f.write("è¯„ä¼°æ–¹æ³•: å¤šç»´åº¦ç»¼åˆè¯„ä¼°ï¼ˆæ¢¯åº¦åŸŸ+é¢‘åŸŸ+å±€éƒ¨è‡ªé€‚åº”ï¼‰\n")
            f.write("å™ªå£°å¤„ç†: æ™ºèƒ½åˆ†ç¦»å¤–æºæ€§å™ªå£°ï¼Œä¿ç•™å†…æºæ€§ç‰¹å¾\n")
            f.write("è®¾è®¡ç›®æ ‡: è§£å†³å™ªå£°å¹²æ‰°ã€ç¼©æ”¾ä¸ä¸€è‡´ã€å±€éƒ¨å¯¹æ¯”åº¦å·®å¼‚\n\n")
            
            f.write("å¤„ç†ç»Ÿè®¡:\n")
            f.write(f"æ€»æ–‡ä»¶æ•°: {stats['total']}\n")
            f.write(f"æˆåŠŸè¯„ä¼°: {stats['success']}\n")
            f.write(f"è¯„ä¼°å¤±è´¥: {stats['failed']}\n")
            f.write(f"æˆåŠŸç‡: {stats['success']/stats['total']*100:.1f}%\n\n")
            
            if results:
                df = pd.DataFrame(results)
                f.write("å¾—åˆ†ç»Ÿè®¡:\n")
                f.write(f"ç»¼åˆå¾—åˆ†èŒƒå›´: {df['total_score'].min():.2f} - {df['total_score'].max():.2f}\n")
                f.write(f"ç»¼åˆå¾—åˆ†å¹³å‡å€¼: {df['total_score'].mean():.2f}\n")
                f.write(f"æ¢¯åº¦å¾—åˆ†å¹³å‡å€¼: {df['gradient_score'].mean():.2f}\n")
                f.write(f"é¢‘åŸŸå¾—åˆ†å¹³å‡å€¼: {df['frequency_score'].mean():.2f}\n")
                f.write(f"å±€éƒ¨å¾—åˆ†å¹³å‡å€¼: {df['local_score'].mean():.2f}\n")
                f.write(f"å™ªå£°å»é™¤æ¯”ä¾‹å¹³å‡å€¼: {df['noise_removed_ratio'].mean():.1%}\n\n")
                
                # è´¨é‡åˆ†å¸ƒ
                f.write("è´¨é‡åˆ†å¸ƒ (åŸºäºå»å™ªåè¯„ä¼°):\n")
                excellent = len(df[df['total_score'] >= 80])
                good = len(df[(df['total_score'] >= 60) & (df['total_score'] < 80)])
                poor = len(df[df['total_score'] < 60])
                f.write(f"ä¼˜ç§€(â‰¥80): {excellent}å¼  ({excellent/len(df)*100:.1f}%)\n")
                f.write(f"è‰¯å¥½(60-79): {good}å¼  ({good/len(df)*100:.1f}%)\n")
                f.write(f"éœ€æ”¹è¿›(<60): {poor}å¼  ({poor/len(df)*100:.1f}%)\n\n")
                
                # å¸¸è§é—®é¢˜åˆ†æ
                f.write("å¸¸è§é—®é¢˜åˆ†æ:\n")
                diagnosis_counts = df['diagnosis'].value_counts()
                for diagnosis, count in diagnosis_counts.items():
                    f.write(f"{diagnosis}: {count}æ¬¡\n")
                
                # æ”¹è¿›å»ºè®®ç»Ÿè®¡
                f.write("\næ”¹è¿›å»ºè®®ç»Ÿè®¡:\n")
                recommendation_counts = df['recommendation'].value_counts()
                for recommendation, count in recommendation_counts.items():
                    f.write(f"{recommendation}: {count}æ¬¡\n")
            
            if stats['failed_files']:
                f.write("\nå¤±è´¥æ–‡ä»¶åˆ—è¡¨:\n")
                for file in stats['failed_files']:
                    f.write(f"- {file}\n")
        
        print(f"\nğŸ“Š å¢å¼ºç‰ˆæ¸…æ™°åº¦æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def _generate_skeleton_report(self, stats):
        """ç”Ÿæˆéª¨æ¶æå–æŠ¥å‘Š"""
        report_path = os.path.join(self.output_dir, "skeleton_report.txt")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("ç”²éª¨æ–‡æ‹“ç‰‡éª¨æ¶æå–æŠ¥å‘Š\n")
            f.write("=" * 40 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è¾“å…¥ç›®å½•: {self.input_dir}\n")
            f.write(f"è¾“å‡ºç›®å½•: {self.output_dir}\n\n")
            f.write("å¤„ç†ç»Ÿè®¡:\n")
            f.write(f"æ€»æ–‡ä»¶æ•°: {stats['total']}\n")
            f.write(f"æˆåŠŸæå–: {stats['success']}\n")
            f.write(f"æå–å¤±è´¥: {stats['failed']}\n")
            f.write(f"æˆåŠŸç‡: {stats['success']/stats['total']*100:.1f}%\n")
            
            if stats['failed_files']:
                f.write("\nå¤±è´¥æ–‡ä»¶åˆ—è¡¨:\n")
                for file in stats['failed_files']:
                    f.write(f"- {file}\n")
        
        print(f"\nğŸ“Š éª¨æ¶æå–æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ç”²éª¨æ–‡æ‹“ç‰‡å¤„ç†ç³»ç»Ÿ - å¢å¼ºç‰ˆ")
    print("=" * 50)
    print("æ–°å¢åŠŸèƒ½: å¤šç»´åº¦æ¸…æ™°åº¦è¯„ä¼°ï¼ˆæŠ—å™ªå£°ã€å°ºåº¦ä¸å˜ã€å±€éƒ¨è‡ªé€‚åº”ï¼‰")
    print("=" * 50)
    
    # é…ç½®è·¯å¾„ - ä¿®æ”¹ä¸ºæ‚¨éœ€è¦çš„è·¯å¾„
    input_directory = "/work/home/succuba/BRISQUE/data/tapian"  # è¾“å…¥ç›®å½•
    output_directory = "/work/home/succuba/BRISQUE/enhanced_results"  # è¾“å‡ºç›®å½•
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(input_directory):
        print(f"é”™è¯¯: è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_directory}")
        print("è¯·ä¿®æ”¹ input_directory å˜é‡")
        return
    
    # åˆ›å»ºå¢å¼ºç‰ˆå¤„ç†å™¨
    processor = EnhancedBatchProcessor(
        input_dir=input_directory,
        output_dir=output_directory
    )
    
    try:
        # ç¬¬ä¸€æ­¥ï¼šå¢å¼ºç‰ˆæ¸…æ™°åº¦è¯„ä¼°
        print("\n" + "=" * 60)
        print("å¼€å§‹å¢å¼ºç‰ˆæ¸…æ™°åº¦è¯„ä¼°")
        print("=" * 60)
        clarity_stats = processor.process_clarity_evaluation()
        
        print("\n" + "=" * 60)
        print("å¢å¼ºç‰ˆæ¸…æ™°åº¦è¯„ä¼°å®Œæˆ!")
        print(f"æˆåŠŸè¯„ä¼°: {clarity_stats['success']}/{clarity_stats['total']} ä¸ªæ–‡ä»¶")
        print(f"æˆåŠŸç‡: {clarity_stats['success']/clarity_stats['total']*100:.1f}%")
        
        # ç¬¬äºŒæ­¥ï¼šéª¨æ¶æå–
        print("\n" + "=" * 60)
        print("å¼€å§‹éª¨æ¶æå–")
        print("=" * 60)
        skeleton_stats = processor.process_skeleton_extraction()
        
        print("\n" + "=" * 60)
        print("éª¨æ¶æå–å®Œæˆ!")
        print(f"æˆåŠŸæå–: {skeleton_stats['success']}/{skeleton_stats['total']} ä¸ªæ–‡ä»¶")
        print(f"æˆåŠŸç‡: {skeleton_stats['success']/skeleton_stats['total']*100:.1f}%")
        print(f"ç»“æœä¿å­˜åœ¨: {output_directory}")
        
    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

# å•æ–‡ä»¶æµ‹è¯•å‡½æ•°
def test_single_file():
    """æµ‹è¯•å•æ–‡ä»¶å¤„ç† - å¢å¼ºç‰ˆï¼ˆä¿å­˜å»å™ªæ•ˆæœå›¾ï¼‰"""
    test_image = "/work/home/succuba/BRISQUE/data/tapian/1/1.png"  # æµ‹è¯•æ–‡ä»¶
    
    if not os.path.exists(test_image):
        print("âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    print("ğŸ§ª å¼€å§‹å•æ–‡ä»¶æµ‹è¯•ï¼ˆä¿å­˜å»å™ªæ•ˆæœå›¾ï¼‰...")
    print("=" * 60)
    
    # 1. é¦–å…ˆå•ç‹¬å¤„ç†å™ªå£°å»é™¤å¹¶ä¿å­˜ç»“æœ
    print("ğŸ” è¿›è¡Œå¤–æºæ€§å™ªå£°å»é™¤...")
    noise_processor = IntelligentNoiseProcessor()
    denoised_img, noise_mask = noise_processor.remove_exogenous_noise(test_image)
    
    if denoised_img is not None and noise_mask is not None:
        # åˆ›å»ºä¸“é—¨çš„è¾“å‡ºç›®å½•ç”¨äºä¿å­˜å»å™ªç»“æœ
        output_dir = "/work/home/succuba/BRISQUE/tmp/oracle_bone_debug"
        os.makedirs(output_dir, exist_ok=True)
        
        # è·å–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        file_name = os.path.splitext(os.path.basename(test_image))[0]
        
        # ä¿å­˜å»å™ªåçš„å›¾åƒ
        denoised_path = f"{output_dir}/{file_name}_denoised.png"
        cv2.imwrite(denoised_path, denoised_img)
        
        # ä¿å­˜å™ªå£°æ©ç ï¼ˆå¯è§†åŒ–å™ªå£°åŒºåŸŸï¼‰
        noise_mask_path = f"{output_dir}/{file_name}_noise_mask.png"
        cv2.imwrite(noise_mask_path, noise_mask)
        
        print(f"âœ… å»å™ªå›¾åƒå·²ä¿å­˜: {denoised_path}")
        print(f"âœ… å™ªå£°æ©ç å·²ä¿å­˜: {noise_mask_path}")
        print(f"ğŸ“ æ‰€æœ‰è°ƒè¯•æ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")
    else:
        print("âŒ å™ªå£°å¤„ç†å¤±è´¥ï¼Œæ— æ³•ä¿å­˜å»å™ªæ•ˆæœå›¾")
        return  # å¦‚æœå™ªå£°å¤„ç†å¤±è´¥ï¼Œæå‰é€€å‡º
    
    print("\n" + "=" * 60)
    print("ğŸ“Š è¿›è¡Œæ¸…æ™°åº¦è¯„ä¼°...")
    print("=" * 60)
    
    # 2. ç»§ç»­è¿›è¡ŒåŸæœ‰çš„æ¸…æ™°åº¦è¯„ä¼°ï¼ˆä½¿ç”¨å»å™ªåçš„å›¾åƒï¼‰
    evaluator = EnhancedClarityEvaluator()
    clarity_result = evaluator.calculate_detailed_assessment(test_image)
    
    if clarity_result:
        print(f"âœ… æ¸…æ™°åº¦è¯„ä¼°æˆåŠŸ!")
        print(f"   ç»¼åˆå¾—åˆ†: {clarity_result['total_score']:.2f}")
        print(f"   æ¢¯åº¦å¾—åˆ†: {clarity_result['detailed_scores']['gradient']:.2f}")
        print(f"   é¢‘åŸŸå¾—åˆ†: {clarity_result['detailed_scores']['frequency']:.2f}")
        print(f"   å±€éƒ¨å¾—åˆ†: {clarity_result['detailed_scores']['local']:.2f}")
        print(f"   å™ªå£°å»é™¤æ¯”ä¾‹: {clarity_result['noise_removed_ratio']:.1%}")
        print(f"   è¯Šæ–­ç»“æœ: {', '.join(clarity_result['diagnosis'])}")
        print(f"   æ”¹è¿›å»ºè®®: {clarity_result['recommendation']}")
    else:
        print("âŒ æ¸…æ™°åº¦è¯„ä¼°å¤±è´¥")
    
    print("\n" + "=" * 60)
    print("ğŸ¦´ è¿›è¡Œéª¨æ¶æå–...")
    print("=" * 60)
    
    # 3. è¿›è¡Œéª¨æ¶æå–
    extractor = OracleBoneSkeletonExtractor()
    skeleton = extractor.extract_skeleton(test_image)
    
    if skeleton is not None:
        skeleton_path = f"{output_dir}/{file_name}_skeleton.png"
        cv2.imwrite(skeleton_path, skeleton)
        print(f"âœ… éª¨æ¶æå–å®Œæˆ! ç»“æœä¿å­˜åˆ°: {skeleton_path}")
        
        # å¯é€‰ï¼šæ˜¾ç¤ºå¤„ç†å‰åçš„å¯¹æ¯”ä¿¡æ¯
        print("\nğŸ“‹ å¤„ç†æ‘˜è¦:")
        print(f"   åŸå§‹å›¾åƒ: {test_image}")
        print(f"   å»å™ªå›¾åƒ: {denoised_path}")
        print(f"   å™ªå£°æ©ç : {noise_mask_path}")
        print(f"   éª¨æ¶å›¾åƒ: {skeleton_path}")
    else:
        print("âŒ éª¨æ¶æå–å¤±è´¥")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ å•æ–‡ä»¶æµ‹è¯•å®Œæˆ! æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°è°ƒè¯•ç›®å½•")

if __name__ == "__main__":
    # è¿è¡Œæ‰¹é‡å¤„ç†
    # main()
    
    # å¦‚æœè¦æµ‹è¯•å•æ–‡ä»¶ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    test_single_file()